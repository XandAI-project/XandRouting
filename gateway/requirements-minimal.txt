# Minimal requirements - only core and llama.cpp
# Add vllm or transformers as needed
fastapi==0.109.0
uvicorn[standard]==0.27.0
pydantic==2.5.0
httpx==0.26.0
llama-cpp-python>=0.2.27
torch>=2.1.2
