# Core API framework
fastapi>=0.128.0,<0.129.0
uvicorn[standard]>=0.30.0,<0.33.0
pydantic>=2.5.0,<3.0.0

# HTTP client
httpx>=0.27.0,<0.28.0

# vLLM inference engine (latest with Qwen3 support)
vllm==0.15.1

# Transformers and ML libraries
transformers>=5.0.0,<5.2.0
torch>=2.5.0,<2.7.0
accelerate>=0.33.0,<0.34.0

# Data processing
numpy>=1.26.0,<2.0.0
sentencepiece>=0.2.0,<0.3.0
protobuf>=5.0.0,<7.0.0

# llama.cpp support
llama-cpp-python>=0.3.0

# HuggingFace Hub
huggingface-hub>=0.25.0,<2.0.0
