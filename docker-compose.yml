version: '3.8'

networks:
  llm-network:
    driver: bridge

volumes:
  models:
    driver: local

services:
  # Gateway Service - Request Router
  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile.gateway
    container_name: llm-gateway
    ports:
      - "8080:8080"  # Only gateway exposed to host
    networks:
      - llm-network
    volumes:
      - ./models:/models  # Read-write for downloads
    environment:
      - PORT=8080
      - HOST=0.0.0.0
      - VLLM_SERVICE=vllm-engine:8000
      - LLAMACPP_SERVICE=llamacpp-engine:8000
      - TRANSFORMERS_SERVICE=transformers-engine:8000
      - CORS_ORIGINS=*
      - CORS_CREDENTIALS=true
      - CORS_METHODS=*
      - CORS_HEADERS=*
    depends_on:
      - vllm-engine
      - llamacpp-engine
      - transformers-engine
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # vLLM Engine Service
  vllm-engine:
    build:
      context: ./workers/vllm
      dockerfile: Dockerfile
    container_name: vllm-engine
    expose:
      - "8000"  # Internal only
    ports:
      - "0:8000"  # Docker assigns random host port
    networks:
      - llm-network
    volumes:
      - ./models:/models:ro  # Read-only access
    environment:
      - PORT=8000
      - HOST=0.0.0.0
      - ENGINE_TYPE=vllm
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    shm_size: '2gb'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # llama.cpp Engine Service
  llamacpp-engine:
    build:
      context: ./workers/llamacpp
      dockerfile: Dockerfile
    container_name: llamacpp-engine
    expose:
      - "8000"  # Internal only
    ports:
      - "0:8000"  # Docker assigns random host port
    networks:
      - llm-network
    volumes:
      - ./models:/models:ro  # Read-only access
    environment:
      - PORT=8000
      - HOST=0.0.0.0
      - ENGINE_TYPE=llamacpp
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Transformers Engine Service
  transformers-engine:
    build:
      context: ./workers/transformers
      dockerfile: Dockerfile
    container_name: transformers-engine
    expose:
      - "8000"  # Internal only
    ports:
      - "0:8000"  # Docker assigns random host port
    networks:
      - llm-network
    volumes:
      - ./models:/models:ro  # Read-only access
    environment:
      - PORT=8000
      - HOST=0.0.0.0
      - ENGINE_TYPE=transformers
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
