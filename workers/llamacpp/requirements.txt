# llama-cpp-python with CUDA support
llama-cpp-python>=0.3.0

# FastAPI for serving
fastapi>=0.128.0,<0.129.0
uvicorn[standard]>=0.30.0,<0.33.0
pydantic>=2.9.0,<3.0.0
